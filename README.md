åŒ…æ‹¬ä½†ä¸é™äºï¼š
- éæ³•è·å–å…¬æ°‘ä¸ªäººä¿¡æ¯
- ç½‘ç»œæ”»å‡»å’Œå…¥ä¾µ
- æ•°æ®çªƒå–å’Œè´©å–
- è¯ˆéª—ç­‰è¿æ³•çŠ¯ç½ªæ´»åŠ¨

ã€
# ğŸš€ ä¸€ä¸ªåŸºäºPythonçš„é«˜æ€§èƒ½ã€é«˜å¹¶å‘ç½‘ç»œçˆ¬è™«ï¼Œä¸“é—¨ç”¨äºæå–ç½‘é¡µä¸­çš„æ‰‹æœºå·ç å’Œèº«ä»½è¯ä¿¡æ¯ã€‚æ”¯æŒå¤šçº¿ç¨‹ã€ä»£ç†æ± ã€è‡ªåŠ¨å»é‡å’Œæ•°æ®éªŒè¯ã€‚

## âœ¨ ç‰¹æ€§

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
- **é«˜å¹¶å‘çˆ¬å–** - å¤šçº¿ç¨‹å¹¶å‘è¯·æ±‚ï¼Œæœ€é«˜æ”¯æŒæ•°ç™¾å¹¶å‘
- **æ™ºèƒ½æ•°æ®æå–** - æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ‰‹æœºå·å’Œèº«ä»½è¯
- **æ•°æ®éªŒè¯** - èº«ä»½è¯å·ç æœ‰æ•ˆæ€§æ ¡éªŒ
- **ä»£ç†æ”¯æŒ** - å†…ç½®å…è´¹ä»£ç†æ± ï¼Œè‡ªåŠ¨ç»´æŠ¤å’ŒéªŒè¯
- **å¤šç§æ•°æ®æº** - æ”¯æŒURLåˆ—è¡¨ã€æ–‡ä»¶ã€æœç´¢å¼•æ“ç­‰å¤šç§æ•°æ®æº

### ğŸ”§ æŠ€æœ¯ç‰¹æ€§
- **è¯·æ±‚é‡è¯•æœºåˆ¶** - è‡ªåŠ¨é‡è¯•å¤±è´¥è¯·æ±‚
- **éšæœºUser-Agent** - é˜²åçˆ¬è™«æ£€æµ‹
- **è¯·æ±‚é¢‘ç‡æ§åˆ¶** - å¯è°ƒèŠ‚çš„è¯·æ±‚å»¶è¿Ÿ
- **è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯** - å®æ—¶ç›‘æ§çˆ¬å–çŠ¶æ€
- **æ•°æ®å»é‡** - è‡ªåŠ¨è¿‡æ»¤é‡å¤æ•°æ®
- **å¤šç§æ ¼å¼å¯¼å‡º** - CSVæ–‡ä»¶æ ¼å¼

## ğŸ›  å®‰è£…ä¾èµ–

```bash
pip install requests beautifulsoup4 fake-useragent aiohttp loguru prettytable retrying
```

## ğŸ“– å¿«é€Ÿå¼€å§‹

### åŸºç¡€ä½¿ç”¨

```python
from crawl import HighConcurrencyCrawler

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = HighConcurrencyCrawler(
    max_workers=20,        # å¹¶å‘çº¿ç¨‹æ•°
    request_delay=0.1,     # è¯·æ±‚å»¶è¿Ÿ(ç§’)
    use_proxies=False      # æ˜¯å¦ä½¿ç”¨ä»£ç†
)

# ä»URLåˆ—è¡¨çˆ¬å–
urls = [
    'https://www.example.com/page1',
    'https://www.example.com/page2'
]

results = crawler.crawl_phone_and_id_from_urls(
    urls, 
    save_to_file='results.csv'
)

print(f"æ‰¾åˆ°æ‰‹æœºå·: {len(results['phones'])} ä¸ª")
print(f"æ‰¾åˆ°èº«ä»½è¯: {len(results['id_cards'])} ä¸ª")
```

### ä»æ–‡ä»¶çˆ¬å–

```python
# ä»æ–‡æœ¬æ–‡ä»¶æå–URLå¹¶çˆ¬å–
results = crawler.crawl_from_file(
    'urls.txt', 
    save_to_file='file_results.csv'
)
```

### è‡ªå®šä¹‰çˆ¬å–

```python
# æ‰‹åŠ¨å¤„ç†å“åº”
def custom_callback(response):
    text = response.text
    # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
    phones = crawler.extract_phone_numbers(text)
    id_cards = crawler.extract_id_cards(text)
    return {
        'phones': phones,
        'id_cards': id_cards,
        'content_length': len(text)
    }

results = crawler.crawl_urls_concurrently(
    urls, 
    callback=custom_callback
)
```

## ğŸ“Š è¾“å‡ºæ ¼å¼

### æ‰‹æœºå·æ•°æ®
```csv
æ‰‹æœºå·,æå–æ—¶é—´
13812345678,2024-01-01 12:00:00
13987654321,2024-01-01 12:00:01
```

### èº«ä»½è¯æ•°æ®
```csv
id_card,area_code,birth_date,age,gender,is_valid,source_url,crawl_time
110101199001011234,110101,1990-01-01,34,ç”·,æ˜¯,https://example.com,2024-01-01 12:00:00
```

## âš™ï¸ é…ç½®é€‰é¡¹

### çˆ¬è™«å‚æ•°
```python
crawler = HighConcurrencyCrawler(
    max_workers=20,        # å¹¶å‘çº¿ç¨‹æ•° (1-100)
    request_delay=0.05,    # è¯·æ±‚é—´éš”(ç§’)
    use_proxies=True,      # å¯ç”¨ä»£ç†æ± 
    proxy_list=['http://proxy1:port', 'http://proxy2:port']  # è‡ªå®šä¹‰ä»£ç†
)
```

### ä»£ç†æ± é…ç½®
```python
from crawl import FreeProxyPool

proxy_pool = FreeProxyPool(
    test_urls=['https://httpbin.org/ip'],  # ä»£ç†æµ‹è¯•URL
    check_interval=300,     # ä»£ç†æ£€æŸ¥é—´éš”(ç§’)
    min_proxies=10,         # æœ€å°ä»£ç†æ•°é‡
    max_proxies=50          # æœ€å¤§ä»£ç†æ•°é‡
)
```

## ğŸ” æ•°æ®æå–è§„åˆ™

### æ‰‹æœºå·åŒ¹é…
- æ ‡å‡†æ ¼å¼: `1[3-9]XXXXXXX`
- å¸¦å›½é™…ç : `+86 1[3-9]XXXXXXX`
- å¸¦åˆ†éš”ç¬¦: `138-1234-5678`

### èº«ä»½è¯åŒ¹é…
- 18ä½èº«ä»½è¯å·ç 
- è¡Œæ”¿åŒºåˆ’ä»£ç éªŒè¯
- å‡ºç”Ÿæ—¥æœŸéªŒè¯
- æ ¡éªŒç éªŒè¯

### æŠ€æœ¯é™åˆ¶
- ğŸ›¡ï¸ æ”¯æŒHTTPSåè®®
- ğŸ”„ è‡ªåŠ¨å¤„ç†ç¼–ç é—®é¢˜
- â° å†…ç½®è¯·æ±‚é¢‘ç‡é™åˆ¶
- âŒ è‡ªåŠ¨è¿‡æ»¤æ— æ•ˆæ•°æ®

### å¸¸è§é—®é¢˜

1. **å¯¼å…¥é”™è¯¯**
   ```bash
   # å®‰è£…ç¼ºå¤±çš„åº“
   pip install beautifulsoup4 fake-useragent
   ```

2. **è¯·æ±‚è¢«é˜»å¡**
   ```python
   # å¢åŠ å»¶è¿Ÿæˆ–ä½¿ç”¨ä»£ç†
   crawler.set_request_delay(0.5)
   crawler.set_proxy_list(proxy_list)
   ```

3. **å†…å­˜å ç”¨è¿‡é«˜**
   ```python
   # å‡å°‘å¹¶å‘æ•°
   crawler.set_max_workers(10)
   ```

### æ—¥å¿—æŸ¥çœ‹
```python
from crawl import log
log.set_level('DEBUG')  # è®¾ç½®æ—¥å¿—çº§åˆ«
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
crawl.py                 # ä¸»çˆ¬è™«æ–‡ä»¶
â”œâ”€â”€ CrawlBase           # çˆ¬è™«åŸºç±»
â”œâ”€â”€ HighConcurrencyCrawler # é«˜å¹¶å‘çˆ¬è™«
â”œâ”€â”€ FreeProxyPool       # ä»£ç†æ± ç®¡ç†
â””â”€â”€ å·¥å…·å‡½æ•°            # æ•°æ®éªŒè¯ã€å¯¼å‡ºç­‰
```
## å·¥å…·æ­£åœ¨æ„æ·«ä¸­...
